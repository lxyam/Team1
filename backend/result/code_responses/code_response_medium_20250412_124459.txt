
{
  "title": "实时电影推荐系统",
  "difficulty": "medium",
  "description": "你需要构建一个实时电影推荐系统的后端服务。系统需要从Kafka流中读取用户行为数据（浏览/评分），使用Spark进行实时处理，将处理后的数据存入MongoDB，并通过机器学习模型（PyTorch）生成个性化推荐。最后通过FastAPI暴露推荐接口。",
  "requirements": [
    "使用Python实现主要逻辑",
    "使用Kafka作为消息队列接收用户行为事件",
    "使用Spark Structured Streaming处理实时数据",
    "将处理后的数据存储到MongoDB",
    "使用PyTorch实现简单的推荐模型",
    "通过FastAPI提供RESTful API接口",
    "实现至少一个推荐接口，根据用户ID返回推荐电影列表"
  ],
  "example_input": {
    "user_id": "u123",
    "user_events": [
      {"movie_id": "m1", "action": "view", "timestamp": "2023-01-01T12:00:00"},
      {"movie_id": "m2", "action": "rate", "rating": 4.5, "timestamp": "2023-01-01T12:05:00"}
    ]
  },
  "example_output": {
    "recommendations": ["m3", "m5", "m8"],
    "generated_at": "2023-01-01T12:10:00.123456"
  },
  "solution": {
    "code": "```python\nimport os\nfrom typing import List\nfrom datetime import datetime\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\nimport pymongo\nimport torch\nimport torch.nn as nn\n\n# 1. 初始化FastAPI应用\napp = FastAPI()\n\n# 2. 定义数据模型\nclass UserEvent(BaseModel):\n    movie_id: str\n    action: str\n    rating: float = None\n    timestamp: datetime\n\nclass RecommendationRequest(BaseModel):\n    user_id: str\n\nclass RecommendationResponse(BaseModel):\n    recommendations: List[str]\n    generated_at: datetime\n\n# 3. 初始化Spark会话\nspark = SparkSession.builder \\\n    .appName(\"MovieRecommendationStream\") \\\n    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/movie_db.user_events\") \\\n    .getOrCreate()\n\n# 4. 定义Kafka数据模式\nschema = StructType([\n    StructField(\"movie_id\", StringType()),\n    StructField(\"action\", StringType()),\n    StructField(\"rating\", FloatType()),\n    StructField(\"timestamp\", TimestampType())\n])\n\n# 5. 从Kafka读取流数据\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"user_events\") \\\n    .load()\n\n# 6. 处理流数据\nparsed_df = df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\n# 7. 写入MongoDB\nquery = parsed_df.writeStream \\\n    .format(\"mongo\") \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n    .start()\n\n# 8. 简单的推荐模型\nclass RecommendationModel(nn.Module):\n    def __init__(self, num_users, num_movies, embedding_dim=10):\n        super().__init__()\n        self.user_embed = nn.Embedding(num_users, embedding_dim)\n        self.movie_embed = nn.Embedding(num_movies, embedding_dim)\n        \n    def forward(self, user_ids):\n        user_embeds = self.user_embed(user_ids)\n        # 这里简化了推荐逻辑，实际应用中会更复杂\n        return torch.argsort(torch.randn(num_movies), descending=True)[:3]\n\n# 9. 初始化模型\nmodel = RecommendationModel(num_users=1000, num_movies=5000)\nmodel.load_state_dict(torch.load(\"model.pt\"))\nmodel.eval()\n\n# 10. FastAPI端点\n@app.post(\"/recommend\", response_model=RecommendationResponse)\nasync def get_recommendations(request: RecommendationRequest):\n    # 连接MongoDB获取用户历史\n    client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n    db = client[\"movie_db\"]\n    history = db.user_events.find({\"user_id\": request.user_id})\n    \n    # 这里简化了用户ID到模型输入的转换\n    user_id_int = hash(request.user_id) % 1000\n    user_id_tensor = torch.tensor([user_id_int], dtype=torch.long)\n    \n    # 生成推荐\n    with torch.no_grad():\n        rec_indices = model(user_id_tensor)\n    \n    # 转换回电影ID\n    movie_ids = [f\"m{idx}\" for idx in rec_indices.tolist()]\n    \n    return {\n        \"recommendations\": movie_ids,\n        \"generated_at\": datetime.now()\n    }\n\n# 11. 启动应用\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```",
    "comments": [
      "使用Spark Structured Streaming处理Kafka流数据",
      "将处理后的数据实时写入MongoDB",
      "使用PyTorch实现简单的推荐模型",
      "通过FastAPI提供RESTful接口",
      "注意在实际应用中需要更复杂的模型和数据处理逻辑",
      "生产环境中需要考虑错误处理、日志记录和性能优化"
    ]
  },
  "test_cases": [
    {
      "input": {
        "user_id": "u123",
        "user_events": [
          {"movie_id": "m1", "action": "view", "timestamp": "2023-01-01T12:00:00"},
          {"movie_id": "m2", "action": "rate", "rating": 4.5, "timestamp": "2023-01-01T12:05:00"}
        ]
      },
      "expected_output": {
        "recommendations": ["m3", "m5", "m8"],
        "generated_at": "2023-01-01T12:10:00.123456"
      }
    },
    {
      "input": {
        "user_id": "u456",
        "user_events": [
          {"movie_id": "m10", "action": "view", "timestamp": "2023-01-01T12:00:00"},
          {"movie_id": "m20", "action": "rate", "rating": 3.0, "timestamp": "2023-01-01T12:05:00"}
        ]
      },
      "expected_output": {
        "recommendations": ["m15", "m22", "m30"],
        "generated_at": "2023-01-01T12:10:00.123456"
      }
    }
  ],
  "technologies": [
    "Python",
    "FastAPI",
    "Kafka",
    "Apache Spark",
    "MongoDB",
    "PyTorch"
  ],
  "key_points": [
    "实时数据处理能力",
    "流处理框架的使用",
    "NoSQL数据库操作",
    "机器学习模型集成",
    "RESTful API设计",
    "系统集成能力"
  ]
}
