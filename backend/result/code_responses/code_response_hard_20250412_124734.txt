
{
  "title": "实时异常交易检测系统",
  "difficulty": "hard",
  "description": "设计并实现一个实时异常交易检测系统，该系统需要处理来自Kafka的实时交易数据流，使用机器学习模型检测异常交易，并将结果存储到数据库同时通过API提供查询接口。系统需要具备高吞吐量、低延迟的特性，并能够处理至少10万TPS的交易数据。",
  "requirements": [
    "使用Kafka作为数据源，模拟产生交易数据流",
    "使用Apache Spark Streaming或Flink进行实时处理",
    "使用PyTorch或TensorFlow实现的预训练模型进行异常检测",
    "将处理结果同时存入PostgreSQL(结构化数据)和Elasticsearch(全文检索)",
    "使用FastAPI提供RESTful API查询接口",
    "系统需要具备水平扩展能力",
    "包含完整的单元测试和集成测试",
    "提供Docker部署方案"
  ],
  "example_input": {
    "kafka_topic": "transactions",
    "sample_message": {
      "transaction_id": "txn_123456",
      "user_id": "user_789",
      "amount": 1500.00,
      "timestamp": "2023-07-15T14:22:35Z",
      "merchant": "Online Store",
      "location": "New York",
      "payment_method": "credit_card"
    }
  },
  "example_output": {
    "api_response": {
      "transaction_id": "txn_123456",
      "is_anomaly": true,
      "anomaly_score": 0.92,
      "anomaly_reason": "unusual_amount_for_user",
      "processed_at": "2023-07-15T14:22:36Z"
    },
    "database_record": {
      "id": 1,
      "transaction_id": "txn_123456",
      "user_id": "user_789",
      "amount": 1500.00,
      "is_anomaly": true,
      "model_version": "v1.2",
      "processed_at": "2023-07-15T14:22:36Z"
    }
  },
  "solution": {
    "overview": "解决方案采用微服务架构，包含数据生产者、流处理器、模型服务和API服务四个组件",
    "components": [
      {
        "name": "data-producer",
        "language": "Python",
        "description": "模拟交易数据并发送到Kafka",
        "code": "import json\nimport random\nfrom datetime import datetime, timedelta\nfrom confluent_kafka import Producer\n\n# Kafka配置\nconf = {'bootstrap.servers': 'kafka:9092'}\nproducer = Producer(conf)\n\ndef generate_transaction():\n    \"\"\"生成模拟交易数据\"\"\"\n    return {\n        \"transaction_id\": f\"txn_{random.randint(100000, 999999)}\",\n        \"user_id\": f\"user_{random.randint(1, 1000)}\",\n        \"amount\": round(random.uniform(10, 2000), 2),\n        \"timestamp\": (datetime.now() - timedelta(minutes=random.randint(0, 60))).isoformat(),\n        \"merchant\": random.choice([\"Online Store\", \"Grocery\", \"Restaurant\", \"Gas Station\"]),\n        \"location\": random.choice([\"New York\", \"London\", \"Tokyo\", \"Berlin\"]),\n        \"payment_method\": random.choice([\"credit_card\", \"debit_card\", \"paypal\"])\n    }\n\ndef delivery_report(err, msg):\n    \"\"\"Kafka发送回调\"\"\"\n    if err is not None:\n        print(f'Message delivery failed: {err}')\n    else:\n        print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n\n# 持续发送数据\nwhile True:\n    data = generate_transaction()\n    producer.produce('transactions', json.dumps(data).encode('utf-8'), callback=delivery_report)\n    producer.poll(0.1)"
      },
      {
        "name": "stream-processor",
        "language": "Python",
        "framework": "PySpark",
        "description": "使用Spark Structured Streaming处理Kafka数据流",
        "code": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n\n# 定义交易数据结构\nschema = StructType([\n    StructField(\"transaction_id\", StringType()),\n    StructField(\"user_id\", StringType()),\n    StructField(\"amount\", DoubleType()),\n    StructField(\"timestamp\", TimestampType()),\n    StructField(\"merchant\", StringType()),\n    StructField(\"location\", StringType()),\n    StructField(\"payment_method\", StringType())\n])\n\n# 创建Spark会话\nspark = SparkSession.builder \\\n    .appName(\"AnomalyDetection\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n    .getOrCreate()\n\n# 从Kafka读取数据流\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"transactions\") \\\n    .load()\n\n# 解析JSON数据\nparsed_df = df.select(\n    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n).select(\"data.*\")\n\n# TODO: 在这里添加模型调用和异常检测逻辑\n\n# 将结果写入PostgreSQL和Elasticsearch\nquery = parsed_df.writeStream \\\n    .foreachBatch(process_batch) \\\n    .outputMode(\"append\") \\\n    .start()\n\nquery.awaitTermination()"
      },
      {
        "name": "model-service",
        "language": "Python",
        "framework": "FastAPI",
        "ml_framework": "PyTorch",
        "description": "提供模型推理服务的FastAPI应用",
        "code": "from fastapi import FastAPI\nimport torch\nimport torch.nn as nn\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n# 定义简单的异常检测模型\nclass AnomalyDetector(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(5, 10)\n        self.fc2 = nn.Linear(10, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.sigmoid(self.fc2(x))\n\n# 加载预训练模型\nmodel = AnomalyDetector()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n\nclass Transaction(BaseModel):\n    transaction_id: str\n    user_id: str\n    amount: float\n    timestamp: str\n    merchant: str\n    location: str\n    payment_method: str\n\n@app.post(\"/detect\")\nasync def detect_anomaly(transaction: Transaction):\n    \"\"\"处理单个交易并返回异常检测结果\"\"\"\n    # 特征工程\n    features = torch.tensor([\n        transaction.amount / 1000,  # 归一化\n        1 if transaction.payment_method == \"credit_card\" else 0,\n        1 if transaction.location == \"New York\" else 0,\n        1 if transaction.merchant == \"Online Store\" else 0,\n        len(transaction.user_id)  # 简单特征\n    ], dtype=torch.float32)\n    \n    # 模型推理\n    with torch.no_grad():\n        score = model(features).item()\n    \n    return {\n        \"transaction_id\": transaction.transaction_id,\n        \"is_anomaly\": score > 0.7,\n        \"anomaly_score\": score,\n        \"anomaly_reason\": \"unusual_amount\" if score > 0.7 else \"normal\"\n    }"
      },
      {
        "name": "api-service",
        "language": "Python",
        "framework": "FastAPI",
        "description": "提供查询接口的FastAPI应用",
        "code": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport psycopg2\nfrom elasticsearch import Elasticsearch\n\napp = FastAPI()\n\n# 数据库连接\npg_conn = psycopg2.connect(\n    dbname=\"transactions\",\n    user=\"postgres\",\n    password=\"password\",\n    host=\"postgres\"\n)\n\nes = Elasticsearch([\"elasticsearch:9200\"])\n\nclass QueryParams(BaseModel):\n    user_id: str = None\n    start_time: str = None\n    end_time: str = None\n    min_amount: float = None\n    max_amount: float = None\n    is_anomaly: bool = None\n\n@app.get(\"/transactions\")\nasync def get_transactions(query: QueryParams):\n    \"\"\"查询交易数据\"\"\"\n    # 构建PostgreSQL查询\n    sql = \"SELECT * FROM transactions WHERE 1=1\"\n    params = []\n    \n    if query.user_id:\n        sql += \" AND user_id = %s\"\n        params.append(query.user_id)\n    \n    if query.start_time and query.end_time:\n        sql += \" AND timestamp BETWEEN %s AND %s\"\n        params.extend([query.start_time, query.end_time])\n    \n    if query.is_anomaly is not None:\n        sql += \" AND is_anomaly = %s\"\n        params.append(query.is_anomaly)\n    \n    # 执行查询\n    cur = pg_conn.cursor()\n    cur.execute(sql, params)\n    results = cur.fetchall()\n    cur.close()\n    \n    return {\"transactions\": results}\n\n@app.get(\"/search\")\nasync def search_transactions(q: str):\n    \"\"\"全文搜索交易数据\"\"\"\n    res = es.search(\n        index=\"transactions\",\n        body={\n            \"query\": {\n                \"multi_match\": {\n                    \"query\": q,\n                    \"fields\": [\"merchant\", \"location\", \"payment_method\"]\n                }\n            }\n        }\n    )\n    return {\"results\": res['hits']['hits']}"
      }
    ],
    "integration": {
      "description": "使用Docker Compose集成所有服务",
      "code": "version: '3.8'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.0.1\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  kafka:\n    image: confluentinc/cp-kafka:7.0.1\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n  data-producer:\n    build:\n      context: ./data-producer\n    depends_on:\n      - kafka\n\n  spark-master:\n    image: bitnami/spark:3.3.0\n    command: \"/opt/bitnami/scripts/spark/run.sh\"\n    environment:\n      - SPARK_MODE=master\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n    ports:\n      - \"8080:8080\"\n\n  spark-worker:\n    image: bitnami/spark:3.3.0\n    command: \"/opt/bitnami/scripts/spark/run.sh\"\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_MEMORY=4G\n      - SPARK_WORKER_CORES=2\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n      - SPARK_SSL_ENABLED=no\n    depends_on:\n      - spark-master\n\n  postgres:\n    image: postgres:13\n    environment:\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: transactions\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - es_data:/usr/share/elasticsearch/data\n\n  model-service:\n    build:\n      context: ./model-service\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - postgres\n      - elasticsearch\n\n  api-service:\n    build:\n      context: ./api-service\n    ports:\n      - \"8001:8001\"\n    depends_on:\n      - postgres\n      - elasticsearch\n\nvolumes:\n  postgres_data:\n  es_data:"
    }
  },
  "test_cases": [
    {
      "description": "测试数据生产者到Kafka",
      "input": "运行data-producer服务",
      "expected_output": "Kafka topic中有持续的交易数据产生"
    },
    {
      "description": "测试Spark流处理",
      "input": "提交Spark作业到集群",
      "expected_output": "Spark UI显示作业正常运行，处理延迟<1秒"
    },
    {
      "description": "测试模型服务API",
      "input": "POST /detect with normal transaction",
      "expected_output": "{\"is_anomaly\": false, \"anomaly_score\": <0.5}"
    },
    {
      "description": "测试查询API",
      "input": "GET /transactions?user_id=user_123",
      "expected_output": "返回该用户的所有交易记录"
    },
    {
      "description": "测试全文搜索",
      "input": "GET /search?q=New+York",
      "expected_output": "返回包含New York的交易记录"
    },
    {
      "description": "压力测试",
      "input": "10万TPS数据流",
      "expected_output": "系统稳定运行，处理延迟<2秒"
    }
  ],
  "technologies": [
    "Python",
    "Apache Kafka",
    "Apache Spark",
    "PostgreSQL",
    "Elasticsearch",
    "FastAPI",
    "PyTorch",
    "Docker"
  ],
  "key_points": [
    "实时流处理架构设计",
    "机器学习模型服务化",
    "多数据存储选型",
    "高并发API设计",
    "分布式系统集成",
    "性能优化",
    "容器化部署"
  ]
}
